/*
 * Copyright (c) 2015, Texas Instruments Incorporated
 * All rights reserved.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions
 * are met:
 *
 * *  Redistributions of source code must retain the above copyright
 *    notice, this list of conditions and the following disclaimer.
 *
 * *  Redistributions in binary form must reproduce the above copyright
 *    notice, this list of conditions and the following disclaimer in the
 *    documentation and/or other materials provided with the distribution.
 *
 * *  Neither the name of Texas Instruments Incorporated nor the names of
 *    its contributors may be used to endorse or promote products derived
 *    from this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,
 * THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR
 * PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR
 * CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL,
 * EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO,
 * PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS;
 * OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,
 * WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR
 * OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE,
 * EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 */
/*
 *  ======== Mmu.c ========
 */

#include <xdc/std.h>
#include <xdc/runtime/Assert.h>
#include <xdc/runtime/Startup.h>

#include <ti/sysbios/BIOS.h>
#include <ti/sysbios/hal/Hwi.h>

#if (ti_sysbios_BIOS_smpEnabled__D)
#include <ti/sysbios/family/arm/a15/smp/Cache.h>
#include <ti/sysbios/family/arm/a15/smp/Core.h>
#else
#include <ti/sysbios/family/arm/a15/Cache.h>
#include <ti/sysbios/family/arm/a15/Core.h>
#endif

#include "package/internal/Mmu.xdc.h"

#define LEVEL3_BLOCK_DESC_SHIFT 12
#define LEVEL2_BLOCK_DESC_SHIFT 21
#define LEVEL1_TABLE_DESC_SHIFT 30

/*
 *  ======== Mmu_startup ========
 */
Void Mmu_startup()
{
    UInt idx;

    /*
     *  When running in SMP mode, the MMU table init should be done
     *  by core 0 (master) only.
     *
     *  If not running in SMP mode, Core_getId() always returns 0,
     *  and the below init code will be run.
     */
    if (Core_getId() == 0) {
        /* zero out the descriptor table entries */
        Mmu_clearTableBuf();

        /*
         *  Initialize the descriptor table based upon static config
         *  This function is generated by the Mmu.xdt file.
         */
        Mmu_initFirstLevelTableBuf();
        Mmu_initSecondLevelTableBuf();
    }

    /* Install MMU translation tables */
    Mmu_init();

    for (idx = 0; idx < 8; idx++) {
        Mmu_writeMAIRAsm(idx,
            ti_sysbios_family_arm_a15_Mmu_mairRegAttr__C[idx]);
    }

    if (Mmu_enableMMU) {        
        /* enable the MMU */
        if (BIOS_smpEnabled) {
            /*
             * Directly call asm function. Mmu_enable() calls Hwi_disable()
             * which should not be called until the cache is enabled.
             */
            Mmu_enableAsm();
        }
        else {
            Mmu_enable();
        }
    }
}

/*
 *  ======== Mmu_disable ========
 *  Function to disable the MMU.
 */
Void Mmu_disable()
{
#if (ti_sysbios_BIOS_smpEnabled__D)
    Assert_isTrue(FALSE, Mmu_A_disableUnsupported);
#else
    UInt16 type;
    UInt   key;

    /* if MMU is alreay disabled, just return */
    if (!(Mmu_isEnabled())) {
        return;
    }
    
    key = Hwi_disable();

    /* get the current enabled bits */
    type = Cache_getEnabled();
    
    if (type & Cache_Type_L1D) {
        /* writeback invalidate all data cache */
        Cache_wbInvAll();
        
        /* drain the write buffer */
        Cache_wait();

        /* disable the L1 data cache */
        Cache_disable(Cache_Type_L1D);
    }
    
    if (type & Cache_Type_L1P) {
        /* invalidate all L1 program cache */
        Cache_invL1pAll();

        /* disable L1P cache */
        Cache_disable(Cache_Type_L1P);
    }
    
    /* disables the MMU */
    Mmu_disableAsm();

    /* Invalidate entire TLB */
    Mmu_tlbInvAll();

    /* set cache back to initial settings */
    Cache_enable(type);

    Hwi_restore(key);
#endif
}

/*
 *  ======== Mmu_enable ========
 *  Function to enable the MMU.
 */
Void Mmu_enable()
{
    UInt16 type;
    UInt   key;

    /* if MMU is already enabled then just return */
    if (Mmu_isEnabled()) {
        return;
    }

    key = Hwi_disable();

    /* get the current enabled bits */
    type = Cache_getEnabled();
    
    if (type & Cache_Type_ALLP) {
        /* invalidate all of L1 program cache */
        Cache_invL1pAll();

        /* disable L1P cache */
        Cache_disable(Cache_Type_ALLP);
    }

    /* Invalidate entire TLB */
    Mmu_tlbInvAll();
    
    /* enables the MMU */
    Mmu_enableAsm();
    
    /* set cache back to initial settings */
    Cache_enable(type);

    Hwi_restore(key);
}

/*
 *  ======== Mmu_setMAIR ========
 */
Void Mmu_setMAIR(UInt attrIndx, UInt attr)
{
    UInt key;
#if (ti_sysbios_BIOS_smpEnabled__D)
    key = Core_notifySpinLock();

    /* Update module state mairRegAttr */
    Mmu_module->mairRegAttr[attrIndx] = attr;

    /* Ensure all memory write ops complete */
    __asm__ __volatile__ (
        "dmb ishst"
        ::: "memory"
    );

    Core_notify(&Mmu_setMAIRCoreX, (UArg)attrIndx,
        (Core_CPUMASK ^ (0x1 << Core_getId())));

    /* Write attr to MAIRn register */
    Mmu_writeMAIRAsm(attrIndx, attr);

    /* Invalidate the entire TLB (broadcast to other cores) */
    Mmu_tlbInvAll();

    Core_notifySpinUnlock(key);
#else
    key = Hwi_disable();

    /* Update module state mairRegAttr */
    Mmu_module->mairRegAttr[attrIndx] = attr;

    /* Write attr to MAIRn register */
    Mmu_writeMAIRAsm(attrIndx, attr);

    /* Invalidate the entire TLB (broadcast to other cores) */
    Mmu_tlbInvAll();

    Hwi_restore(key);
#endif
}

/*
 *  ======== Mmu_setMAIRCoreX ========
 */
Void Mmu_setMAIRCoreX(UArg arg)
{
    UInt key;
    UInt attrIndx = (UInt)arg;

    key = Core_hwiDisable();

    /* Write attr to MAIRn register */
    Mmu_writeMAIRAsm(attrIndx, Mmu_module->mairRegAttr[attrIndx]);

    /* No need to invalidate TLB as Core0 will broadcast TLB inv */

    Core_hwiRestore(key);
}

/*
 *  ======== Mmu_initDescAttrs ========
 */
Void Mmu_initDescAttrs(Mmu_DescriptorAttrs *attrs)
{
    /* Assert that attrs != NULL */
    Assert_isTrue(attrs != NULL, Mmu_A_nullPointer);

    attrs->type = Mmu_defaultAttrs.type;
    attrs->nsTable = Mmu_defaultAttrs.nsTable;
    attrs->apTable = Mmu_defaultAttrs.apTable;
    attrs->xnTable = Mmu_defaultAttrs.xnTable;
    attrs->pxnTable = Mmu_defaultAttrs.pxnTable;
    attrs->noExecute = Mmu_defaultAttrs.noExecute;
    attrs->privNoExecute = Mmu_defaultAttrs.privNoExecute;
    attrs->contiguous = Mmu_defaultAttrs.contiguous;
    attrs->notGlobal = Mmu_defaultAttrs.notGlobal;
    attrs->accessFlag = Mmu_defaultAttrs.accessFlag;
    attrs->shareable = Mmu_defaultAttrs.shareable;
    attrs->accPerm = Mmu_defaultAttrs.accPerm;
    attrs->nonSecure = Mmu_defaultAttrs.nonSecure;
    attrs->attrIndx = Mmu_defaultAttrs.attrIndx;
    attrs->reserved = Mmu_defaultAttrs.reserved;
}

/*
 *  ======== Mmu_flushiLevel1PageTable ========
 */
Void Mmu_flushLevel1PageTable(Ptr virtualAddr, UInt64 desc)
{
    UInt   i;
    UInt64 *secondLevelTable;
    SizeT  blockSize;

    switch (desc & 0x3) {
        /* Page table descriptor */
        case Mmu_DescriptorType_TABLE:
            blockSize = (SizeT)(1 << LEVEL2_BLOCK_DESC_SHIFT);
            secondLevelTable = (UInt64 *)((UInt32)desc & 0xFFFFF000);
            for (i = 0; i < Mmu_NUM_LEVEL2_ENTRIES; i++) {
                desc = *(secondLevelTable + i);
                Mmu_flushLevel2PageTable((virtualAddr +
                (i << LEVEL2_BLOCK_DESC_SHIFT)), desc);
            }
            break;
        /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            blockSize = (SizeT)(1 << LEVEL1_TABLE_DESC_SHIFT);
            Cache_wbInv(virtualAddr, blockSize, Cache_Type_ALLD, TRUE);
            break;
        default:
            /* do nothing if virtual address not mapped */
            break;
    }
}

/*
 *  ======== Mmu_flushiLevel2PageTable ========
 */
Void Mmu_flushLevel2PageTable(Ptr virtualAddr, UInt64 desc)
{
    UInt   i;
    UInt64 *thirdLevelTable;
    SizeT  blockSize;

    switch (desc & 0x3) {
        /* Page table descriptor */
        case Mmu_DescriptorType_TABLE:
            blockSize = (SizeT)(1 << LEVEL3_BLOCK_DESC_SHIFT);
            thirdLevelTable = (UInt64 *)((UInt)desc & 0xFFFFF000);
            for (i = 0; i < Mmu_NUM_LEVEL2_ENTRIES; i++) {
                desc = *(thirdLevelTable + i);
                if ((desc & 0x3) == 0x3) {
                    /* Flush if entry is a section descriptor */
                    Cache_wbInv((virtualAddr + (i << LEVEL3_BLOCK_DESC_SHIFT)),
                        blockSize, Cache_Type_ALLD, TRUE);
                }
            }
            break;
        /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            blockSize = (SizeT)(1 << LEVEL2_BLOCK_DESC_SHIFT);
            Cache_wbInv(virtualAddr, blockSize, Cache_Type_ALLD, TRUE);
            break;
        default:
            /* do nothing if virtual address not mapped */
            break;
    }
}

/*
 *  ======== Mmu_getFirstLevelTableAddr ========
 */
Ptr Mmu_getFirstLevelTableAddr()
{
    return (Mmu_module->firstLevelTableBuf);
}

/*
 *  ======== Mmu_getSecondLevelTableAddr ========
 */
Ptr Mmu_getSecondLevelTableAddr(UInt8 idx)
{
    return (Mmu_module->secondLevelTableBuf[idx]);
}

/*
 *  ======== Mmu_setFirstLevelDesc ========
 */
Void Mmu_setFirstLevelDesc(Ptr virtualAddr, UInt64 phyAddr,
                             Mmu_DescriptorAttrs *attrs)
{
#if (ti_sysbios_BIOS_smpEnabled__D)
    UInt   key;
    UInt32 index = (UInt32)virtualAddr >> LEVEL1_TABLE_DESC_SHIFT;
    UInt64 desc;
    
    /* Assert that attrs != NULL */
    Assert_isTrue(attrs != NULL, Mmu_A_nullPointer);
    
    key = Core_notifySpinLock();

    /*
     * Writeback invalidate data cache. We need to flush the cache before
     * updating the MMU table since some caches rely on the MMU table
     * to determine the VA->PA mapping when they are being flushed.
     */
    Mmu_flushLevel1PageTable(virtualAddr,
        Mmu_module->firstLevelTableBuf[index]);

    /* Determine which kind of descriptor. */
    switch (attrs->type) {
        case Mmu_DescriptorType_TABLE:
            /* For TABLE descriptors determine second level table address */
            phyAddr = (UInt32)(Mmu_module->secondLevelTableBuf[index]);
            /* Page table descriptor */
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)phyAddr & 0xFFFFFFF000) |
                   ((UInt64)(attrs->pxnTable & 0x1) << 59) |
                   ((UInt64)(attrs->xnTable & 0x1) << 60) |
                   ((UInt64)(attrs->apTable & 0x3) << 61) |
                   ((UInt64)(attrs->nsTable & 0x1) << 63);
            break;
            
        /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)(attrs->attrIndx & 0x7) << 2) |
                   ((UInt64)(attrs->nonSecure & 0x1) << 5) |
                   ((UInt64)(attrs->accPerm & 0x3) << 6) |
                   ((UInt64)(attrs->shareable & 0x3) << 8) |
                   ((UInt64)(attrs->accessFlag & 0x1) << 10) |
                   ((UInt64)(attrs->notGlobal & 0x1) << 11) |
                   ((UInt64)phyAddr & 0xFFC0000000) |
                   ((UInt64)(attrs->contiguous & 0x1) << 52) |
                   ((UInt64)(attrs->privNoExecute & 0x1) << 53) |
                   ((UInt64)(attrs->noExecute & 0x1) << 54) |
                   ((UInt64)(attrs->reserved & 0xF) << 55);
            break;
            
        default:
            Assert_isTrue(FALSE, Mmu_A_unknownDescType);
            break;
    }
    
    /* set the entry in the first level descriptor table */
    Mmu_module->firstLevelTableBuf[index] = desc;

    /*
     * Flush MMU table as translation table walks may not lookup
     * Cache.
     */
    Cache_wbInv(&Mmu_module->firstLevelTableBuf[index],
        4, Cache_Type_ALLD, TRUE);

    /* Invalidate virtual memory range mapped by this entry */
    Mmu_tlbInv(virtualAddr, (SizeT)(1 << LEVEL1_TABLE_DESC_SHIFT));

    /* Errata 798181: do a dummy TLBIMVAIS */
    if ((Mmu_errata798181 == TRUE) &&
        (Core_getRevisionNumber() < 0x40)) {
        __asm__ __volatile__ (
                "mcr p15, #0, r0, c8, c3, #1\n\t"
                "dsb\n\t"
                "clrex\n\t"
                "dmb"
                ::: "memory"
                );
    }

    /* Invalidate program/instruction cache */
    Cache_invL1pAll();

    /* Invalidate all branch predictors */
    Cache_invBPAll();

    /* Flush the instruction pipeline. */
    __asm__ __volatile__ (
            "isb"
            ::: "memory"
            );

    /*
     *  Ensure table changes visible to instruction fetches on all other cores
     */
    Core_notify(&Mmu_instructionSync, 0,
        (Core_CPUMASK ^ (0x1 << Core_getId())));

    Core_notifySpinUnlock(key);
#else
    UInt32 index = (UInt32)virtualAddr >> LEVEL1_TABLE_DESC_SHIFT;
    UInt64 desc = 0;
    Bool   enabled;

    /* Assert that attrs != NULL */
    Assert_isTrue(attrs != NULL, Mmu_A_nullPointer);

    /* determine the current state of the MMU */
    enabled = Mmu_isEnabled();

    /* disable the MMU (if already disabled, does nothing) */
    Mmu_disable();

    /* Determine which kind of descriptor. */
    switch (attrs->type) {
        case Mmu_DescriptorType_TABLE:
            /* For TABLE descriptors determine second level table address */
            phyAddr = (UInt32)(Mmu_module->secondLevelTableBuf[index]);
            /* Page table descriptor */
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)phyAddr & 0xFFFFFFF000) |
                   ((UInt64)(attrs->pxnTable & 0x1) << 59) |
                   ((UInt64)(attrs->xnTable & 0x1) << 60) |
                   ((UInt64)(attrs->apTable & 0x3) << 61) |
                   ((UInt64)(attrs->nsTable & 0x1) << 63);
            break;

        /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)(attrs->attrIndx & 0x7) << 2) |
                   ((UInt64)(attrs->nonSecure & 0x1) << 5) |
                   ((UInt64)(attrs->accPerm & 0x3) << 6) |
                   ((UInt64)(attrs->shareable & 0x3) << 8) |
                   ((UInt64)(attrs->accessFlag & 0x1) << 10) |
                   ((UInt64)(attrs->notGlobal & 0x1) << 11) |
                   ((UInt64)phyAddr & 0xFFC0000000) |
                   ((UInt64)(attrs->contiguous & 0x1) << 52) |
                   ((UInt64)(attrs->privNoExecute & 0x1) << 53) |
                   ((UInt64)(attrs->noExecute & 0x1) << 54) |
                   ((UInt64)(attrs->reserved & 0xF) << 55);
            break;

        default:
            Assert_isTrue(FALSE, Mmu_A_unknownDescType);
            break;
    }

    /* set the entry in the first level descriptor table */
    Mmu_module->firstLevelTableBuf[index] = desc;

    if (enabled) {
        /* if Mmu was enabled, then re-enable it */
        Mmu_enable();
    }

    /* Errata 798181: do a dummy TLBIMVAIS */
    if ((Mmu_errata798181 == TRUE) &&
        (Core_getRevisionNumber() < 0x40)) {
        __asm__ __volatile__ (
                "mcr p15, #0, r0, c8, c3, #1\n\t"
                "dsb\n\t"
                "clrex\n\t"
                "dmb"
                ::: "memory"
                );
    }
#endif
}

/*
 *  ======== Mmu_setSecondLevelDesc ========
 */
Void Mmu_setSecondLevelDesc(Ptr virtualAddr, UInt64 phyAddr,
                              Mmu_DescriptorAttrs *attrs)
{
#if (ti_sysbios_BIOS_smpEnabled__D)
    UInt   key;
    UInt64 desc;
    UInt32 index = (UInt32)virtualAddr >> LEVEL2_BLOCK_DESC_SHIFT;

    /* Assert that attrs != NULL */
    Assert_isTrue(attrs != NULL, Mmu_A_nullPointer);

    key = Core_notifySpinLock();

    /*
     * Writeback invalidate data cache. We need to flush the cache before
     * updating the MMU table since some caches rely on the MMU table
     * to determine the VA->PA mapping when they are being flushed.
     */
    Mmu_flushLevel2PageTable(virtualAddr,
        Mmu_module->secondLevelTableBuf[index >> 9][index & 0x1FF]);

    /* Determine which kind of descriptor. */
    switch (attrs->type) {
        /* Page table descriptor */
        case Mmu_DescriptorType_TABLE:
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)phyAddr & 0xFFFFFFF000) |
                   ((UInt64)(attrs->pxnTable & 0x1) << 59) |
                   ((UInt64)(attrs->xnTable & 0x1) << 60) |
                   ((UInt64)(attrs->apTable & 0x3) << 61) |
                   ((UInt64)(attrs->nsTable & 0x1) << 63);
            break;

        /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)(attrs->attrIndx & 0x7) << 2) |
                   ((UInt64)(attrs->nonSecure & 0x1) << 5) |
                   ((UInt64)(attrs->accPerm & 0x3) << 6) |
                   ((UInt64)(attrs->shareable & 0x3) << 8) |
                   ((UInt64)(attrs->accessFlag & 0x1) << 10) |
                   ((UInt64)(attrs->notGlobal & 0x1) << 11) |
                   ((UInt64)phyAddr & 0xFFFFE00000) |
                   ((UInt64)(attrs->contiguous & 0x1) << 52) |
                   ((UInt64)(attrs->privNoExecute & 0x1) << 53) |
                   ((UInt64)(attrs->noExecute & 0x1) << 54) |
                   ((UInt64)(attrs->reserved & 0xF) << 55);
            break;

        default:
            Assert_isTrue(FALSE, Mmu_A_unknownDescType);
            break;
    }

    /* Ensure all memory operations complete before updating page tables */
    __asm__ __volatile__ (
            "dsb"
            ::: "memory"
            );

    /* set the entry in the second level descriptor table */
    Mmu_module->secondLevelTableBuf[index >> 9][index & 0x1FF] = desc;

    /*
     * Flush MMU table as translation table walks do not lookup Cache.
     */
    Cache_wbInv(&Mmu_module->secondLevelTableBuf[index >> 9][index & 0x1FF],
        4, Cache_Type_ALLD, TRUE);

    /* Invalidate virtual memory range mapped by this entry */
    Mmu_tlbInv(virtualAddr, (SizeT)(1 << LEVEL2_BLOCK_DESC_SHIFT));

    /* Errata 798181: do a dummy TLBIMVAIS, clrex, dmb */
    if ((Mmu_errata798181 == TRUE) &&
        (Core_getRevisionNumber() < 0x40)) {
        __asm__ __volatile__ (
                "mcr p15, #0, r0, c8, c3, #1\n\t"
                "dsb\n\t"
                "clrex\n\t"
                "dmb"
                ::: "memory"
                );
    }

    /* Invalidate program/instruction cache */
    Cache_invL1pAll();

    /* Invalidate all branch predictors */
    Cache_invBPAll();

    /* Flush the instruction pipeline. */
    __asm__ __volatile__ (
            "isb"
            ::: "memory"
            );

    /*
     *  Ensure table changes visible to instruction fetches on all other cores
     */
    Core_notify(&Mmu_instructionSync, 0,
        (Core_CPUMASK ^ (0x1 << Core_getId())));

    Core_notifySpinUnlock(key);
#else
    UInt32 index = (UInt32)virtualAddr >> LEVEL2_BLOCK_DESC_SHIFT;
    UInt64 desc = 0;
    Bool   enabled;

    /* Assert that attrs != NULL */
    Assert_isTrue(attrs != NULL, Mmu_A_nullPointer);

    /* determine the current state of the MMU */
    enabled = Mmu_isEnabled();

    /* disable the MMU (if already disabled, does nothing) */
    Mmu_disable();

    /* Determine which kind of descriptor. */
    switch (attrs->type) {
        case Mmu_DescriptorType_TABLE:
            /* Page table descriptor */
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)phyAddr & 0xFFFFFFF000) |
                   ((UInt64)(attrs->pxnTable & 0x1) << 59) |
                   ((UInt64)(attrs->xnTable & 0x1) << 60) |
                   ((UInt64)(attrs->apTable & 0x3) << 61) |
                   ((UInt64)(attrs->nsTable & 0x1) << 63);
            break;

            /* Section descriptor */
        case Mmu_DescriptorType_BLOCK:
            desc = ((UInt64)attrs->type & 0x3) |
                   ((UInt64)(attrs->attrIndx & 0x7) << 2) |
                   ((UInt64)(attrs->nonSecure & 0x1) << 5) |
                   ((UInt64)(attrs->accPerm & 0x3) << 6) |
                   ((UInt64)(attrs->shareable & 0x3) << 8) |
                   ((UInt64)(attrs->accessFlag & 0x1) << 10) |
                   ((UInt64)(attrs->notGlobal & 0x1) << 11) |
                   ((UInt64)phyAddr & 0xFFFFE00000) |
                   ((UInt64)(attrs->contiguous & 0x1) << 52) |
                   ((UInt64)(attrs->privNoExecute & 0x1) << 53) |
                   ((UInt64)(attrs->noExecute & 0x1) << 54) |
                   ((UInt64)(attrs->reserved & 0xF) << 55);
            break;

        default:
            Assert_isTrue(FALSE, Mmu_A_unknownDescType);
            break;
    }

    /* set the entry in the first level descriptor table */
    Mmu_module->secondLevelTableBuf[index >> 9][index & 0x1FF] = desc;

    if (enabled) {
        /* if Mmu was enabled, then re-enable it */
        Mmu_enable();
    }

    /* Errata 798181: do a dummy TLBIMVAIS */
    if ((Mmu_errata798181 == TRUE) &&
        (Core_getRevisionNumber() < 0x40)) {
        __asm__ __volatile__ (
                "mcr p15, #0, r0, c8, c3, #1\n\t"
                "dsb\n\t"
                "clrex\n\t"
                "dmb"
                ::: "memory"
                );
    }
#endif
}
